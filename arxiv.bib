
@article{cauchy1847methode,
	author = {Cauchy, Augustin and others},
	journal = {Comp. Rend. Sci. Paris},
	number = {1847},
	pages = {536--538},
	title = {General method for solving systems of equations simultaneously},
	volume = {25},
	year = {1847}}

@inproceedings{initialization_and_momentum_in_deep_learning,
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.   },
	address = {Atlanta, Georgia, USA},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	booktitle = {Proceedings of the 30th International Conference on Machine Learning},
	editor = {Dasgupta, Sanjoy and McAllester, David},
	month = {17--19 Jun},
	number = {3},
	pages = {1139--1147},
	pdf = {http://proceedings.mlr.press/v28/sutskever13.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {On the importance of initialization and momentum in deep learning},
	url = {https://proceedings.mlr.press/v28/sutskever13.html},
	volume = {28},
	year = {2013},
	bdsk-url-1 = {https://proceedings.mlr.press/v28/sutskever13.html}}

@article{HeavyBall,
	author = {Polyak, Boris},
	doi = {10.1016/0041-5553(64)90137-5},
	journal = {Ussr Computational Mathematics and Mathematical Physics},
	month = {12},
	pages = {1-17},
	title = {Some methods of speeding up the convergence of iteration methods},
	volume = {4},
	year = {1964},
	bdsk-url-1 = {https://doi.org/10.1016/0041-5553(64)90137-5}}

@article{NesterovMomentum,
	author = {Yurii Nesterov},
	journal = {Proceedings of the USSR Academy of Sciences},
	pages = {543--547},
	title = {A method for unconstrained convex minimization problem with the rate of convergence $o(1/k^2)$},
	volume = {269},
	year = {1983}}

@misc{kingma2017Adam,
	archiveprefix = {arXiv},
	author = {Diederik P. Kingma and Jimmy Ba},
	eprint = {1412.6980},
	primaryclass = {cs.LG},
	title = {Adam: A Method for Stochastic Optimization},
	year = {2017}}

@misc{ODEAdaptiveAlgorithm,
	archiveprefix = {arXiv},
	author = {Andr{\'e} Belotto da Silva and Maxime Gazeau},
	eprint = {1810.13108},
	primaryclass = {cs.LG},
	title = {A general system of differential equations to model first order adaptive algorithms},
	year = {2019}}

@book{Hairerbook,
	author = {Hairer, Ernst and Norsett, Syvert and Wanner, G.},
	doi = {10.1007/978-3-540-78862-1},
	isbn = {978-3-540-56670-0},
	month = {01},
	title = {Solving Ordinary Differential Equations I: Nonstiff Problems},
	volume = {8},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-540-78862-1}}

@inproceedings{xavier_initialization,
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	address = {Chia Laguna Resort, Sardinia, Italy},
	author = {Glorot, Xavier and Bengio, Yoshua},
	booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	editor = {Teh, Yee Whye and Titterington, Mike},
	month = {13--15 May},
	pages = {249--256},
	pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {https://proceedings.mlr.press/v9/glorot10a.html},
	volume = {9},
	year = {2010},
	bdsk-url-1 = {https://proceedings.mlr.press/v9/glorot10a.html}}

@article{StochasticApproximationMethod,
	author = {Herbert Robbins and Sutton Monro},
	doi = {10.1214/aoms/1177729586},
	journal = {The Annals of Mathematical Statistics},
	number = {3},
	pages = {400 -- 407},
	publisher = {Institute of Mathematical Statistics},
	title = {{A Stochastic Approximation Method}},
	url = {https://doi.org/10.1214/aoms/1177729586},
	volume = {22},
	year = {1951},
	bdsk-url-1 = {https://doi.org/10.1214/aoms/1177729586}}

@inbook{Bengio_PracticalRecommendationforTraining,
	abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyperparameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
	address = {Berlin, Heidelberg},
	author = {Bengio, Yoshua},
	booktitle = {Neural Networks: Tricks of the Trade: Second Edition},
	doi = {10.1007/978-3-642-35289-8_26},
	editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
	isbn = {978-3-642-35289-8},
	pages = {437--478},
	publisher = {Springer Berlin Heidelberg},
	title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
	url = {https://doi.org/10.1007/978-3-642-35289-8_26},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-35289-8_26}}

@article{Adagard,
	author = {John Duchi and Elad Hazan and Yoram Singer},
	journal = {Journal of Machine Learning Research},
	number = {61},
	pages = {2121--2159},
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	url = {http://jmlr.org/papers/v12/duchi11a.html},
	volume = {12},
	year = {2011},
	bdsk-url-1 = {http://jmlr.org/papers/v12/duchi11a.html}}

@misc{Adadelta,
	author = {Zeiler, Matthew D.},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1212.5701},
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {ADADELTA: An Adaptive Learning Rate Method},
	url = {https://arxiv.org/abs/1212.5701},
	year = {2012},
	bdsk-url-1 = {https://arxiv.org/abs/1212.5701},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1212.5701}}

@article{GarkSandu,
	abstract = {This work considers a general structure of the additively partitioned Runge--Kutta methods by allowing for different stage values as arguments of different components of the right-hand side. An order conditions theory is developed for the new formulation of generalized additive methods, and stability and monotonicity investigations are carried out. The paper discusses the construction and properties of implicit-explicit and implicit-implicit methods in the new framework. The new approach, named GARK, introduces additional flexibility when compared to traditional partitioned Runge--Kutta formalism and therefore offers additional opportunities for the development of flexible solvers for systems with multiple scales, or driven by multiple physical processes.},
	author = {Adrian Sandu and Michael G\"{u}nther},
	date-modified = {2022-10-13 10:38:30 -0400},
	issn = {00361429},
	journal = {SIAM Journal on Numerical Analysis},
	number = {1},
	pages = {17--42},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {A GENERALIZED-STRUCTURE APPROACH TO ADDITIVE {R}UNGE--{K}UTTA METHODS},
	url = {http://www.jstor.org/stable/24512240},
	urldate = {2022-09-07},
	volume = {53},
	year = {2015},
	bdsk-url-1 = {http://www.jstor.org/stable/24512240}}

@article{InitializationSummary,
	abstract = {Over the past few years, neural networks have exhibited remarkable results for various applications in machine learning and computer vision. Weight initialization is a significant step employed before training any neural network. The weights of a network are initialized and then adjusted repeatedly while training the network. This is done till the loss converges to a minimum value and an ideal weight matrix is obtained. Thus weight initialization directly drives the convergence of a network. Therefore, the selection of an appropriate weight initialization scheme becomes necessary for end-to-end training. An appropriate technique initializes the weights such that the training of the network is accelerated and the performance is improved. This paper discusses various advances in weight initialization for neural networks. The weight initialization techniques in the literature adopted for feed-forward neural network, convolutional neural network, recurrent neural network and long short term memory network have been discussed in this paper. These techniques are classified as (1) initialization techniques without pre-training, which are further classified into random initialization and data-driven initialization, (2) initialization techniques with pre-training. The different weight initialization and weight optimization techniques which select optimal weights for non-iterative training mechanism have also been discussed. We provide a close overview of different initialization schemes in these categories. This paper concludes with discussions on existing schemes and the future scope for research.},
	address = {USA},
	author = {Narkhede, Meenal V. and Bartakke, Prashant P. and Sutaone, Mukul S.},
	doi = {10.1007/s10462-021-10033-z},
	issn = {0269-2821},
	issue_date = {Jan 2022},
	journal = {Artif. Intell. Rev.},
	keywords = {Unsupervised pre-training, Weight initialization, Data-driven initialization, Variance scaling, Interval based, Random initialization},
	month = {jan},
	number = {1},
	numpages = {32},
	pages = {291--322},
	publisher = {Kluwer Academic Publishers},
	title = {A Review on Weight Initialization Strategies for Neural Networks},
	url = {https://doi.org/10.1007/s10462-021-10033-z},
	volume = {55},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s10462-021-10033-z}}

@inproceedings{glorotinitialization,
	abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	address = {Chia Laguna Resort, Sardinia, Italy},
	author = {Glorot, Xavier and Bengio, Yoshua},
	booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	editor = {Teh, Yee Whye and Titterington, Mike},
	month = {13--15 May},
	pages = {249--256},
	pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Understanding the difficulty of training deep feedforward neural networks},
	url = {https://proceedings.mlr.press/v9/glorot10a.html},
	volume = {9},
	year = {2010},
	bdsk-url-1 = {https://proceedings.mlr.press/v9/glorot10a.html}}

@inproceedings{cycliclearningrate,
	author = {Smith, Leslie N.},
	booktitle = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
	doi = {10.1109/WACV.2017.58},
	pages = {464-472},
	title = {Cyclical Learning Rates for Training Neural Networks},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/WACV.2017.58}}

@article{rmsprop,
	author = {Tieleman, Tijmen and Hinton, Geoffrey and others},
	journal = {COURSERA: Neural networks for machine learning},
	number = {2},
	pages = {26--31},
	title = {Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
	volume = {4},
	year = {2012}}

@article{MNIST,
	author = {LECUN, Y.},
	year = {2012},
	journal = {http://yann.lecun.com/exdb/mnist/},
	title = {THE MNIST DATABASE of handwritten digits},
	url = {https://cir.nii.ac.jp/crid/1571417126193283840},
	bdsk-url-1 = {https://cir.nii.ac.jp/crid/1571417126193283840}}

@article{Lorenz63,
	address = {Boston MA, USA},
	author = {Edward N. Lorenz},
	doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
	journal = {Journal of Atmospheric Sciences},
	number = {2},
	pages = {130 - 141},
	publisher = {American Meteorological Society},
	title = {Deterministic Nonperiodic Flow},
	url = {https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml},
	volume = {20},
	year = {1963},
	bdsk-url-1 = {https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml},
	bdsk-url-2 = {https://doi.org/10.1175/1520-0469(1963)020%3C0130:DNF%3E2.0.CO;2}}

@article{HORNIK1989,
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
	doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
	number = {5},
	pages = {359-366},
	title = {Multilayer feedforward networks are universal approximators},
	url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	volume = {2},
	year = {1989},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
	bdsk-url-2 = {https://doi.org/10.1016/0893-6080(89)90020-8}}

@article{HORNIK1991,
	abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.},
	author = {Kurt Hornik},
	doi = {https://doi.org/10.1016/0893-6080(91)90009-T},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation},
	number = {2},
	pages = {251-257},
	title = {Approximation capabilities of multilayer feedforward networks},
	url = {https://www.sciencedirect.com/science/article/pii/089360809190009T},
	volume = {4},
	year = {1991},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/089360809190009T},
	bdsk-url-2 = {https://doi.org/10.1016/0893-6080(91)90009-T}}

@article{Cybenko1991,
	abstract = {{In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function of n real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.}},
	added-at = {2012-03-02T03:39:18.000+0100},
	author = {Cybenko, G.},
	biburl = {https://www.bibsonomy.org/bibtex/2be85c56ae384216b2e35bdf79b7fb477/baby9992006},
	citeulike-article-id = {3561150},
	citeulike-linkout-0 = {http://dx.doi.org/10.1007/BF02551274},
	citeulike-linkout-1 = {http://www.springerlink.com/content/n873j15736072427},
	day = 1,
	doi = {10.1007/BF02551274},
	interhash = {96aecb02daa11041489259a8edb54070},
	intrahash = {be85c56ae384216b2e35bdf79b7fb477},
	issn = {0932-4194},
	journal = {Mathematics of Control, Signals, and Systems (MCSS)},
	keywords = {approximation, control, duckling, free, lunch, no, theorem, theory, ugly, universal},
	month = dec,
	number = 4,
	pages = {303--314},
	posted-at = {2012-02-28 13:17:08},
	priority = {2},
	publisher = {Springer London},
	timestamp = {2012-03-02T03:39:20.000+0100},
	title = {{Approximation by superpositions of a sigmoidal function}},
	url = {http://dx.doi.org/10.1007/BF02551274},
	volume = 2,
	year = 1989,
	bdsk-url-1 = {http://dx.doi.org/10.1007/BF02551274}}

@article{Empirical_Comparisons_of_Optimizers,
	author = {Dami Choi and Christopher J. Shallue and Zachary Nado and Jaehoon Lee and Chris J. Maddison and George E. Dahl},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1910-05446.bib},
	eprint = {1910.05446},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Wed, 16 Oct 2019 16:25:53 +0200},
	title = {On Empirical Comparisons of Optimizers for Deep Learning},
	url = {http://arxiv.org/abs/1910.05446},
	volume = {abs/1910.05446},
	year = {2019},
	bdsk-url-1 = {http://arxiv.org/abs/1910.05446}}

@misc{ELU2016,
	author = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1511.07289},
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
	url = {https://arxiv.org/abs/1511.07289},
	year = {2015},
	bdsk-url-1 = {https://arxiv.org/abs/1511.07289},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1511.07289}}

@misc{SiLU2017,
	author = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1702.03118},
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},
	url = {https://arxiv.org/abs/1702.03118},
	year = {2017},
	bdsk-url-1 = {https://arxiv.org/abs/1702.03118},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1702.03118}}

@misc{GeLU2020,
	author = {Hendrycks, Dan and Gimpel, Kevin},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1606.08415},
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Gaussian Error Linear Units (GELUs)},
	url = {https://arxiv.org/abs/1606.08415},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/abs/1606.08415},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1606.08415}}

@article{BUTCHEROverview2000,
	abstract = {Numerical methods for the solution of initial value problems in ordinary differential equations made enormous progress during the 20th century for several reasons. The first reasons lie in the impetus that was given to the subject in the concluding years of the previous century by the seminal papers of Bashforth and Adams for linear multistep methods and Runge for Runge--Kutta methods. Other reasons, which of course apply to numerical analysis in general, are in the invention of electronic computers half way through the century and the needs in mathematical modelling of efficient numerical algorithms as an alternative to classical methods of applied mathematics. This survey paper follows many of the main strands in the developments of these methods, both for general problems, stiff systems, and for many of the special problem types that have been gaining in significance as the century draws to an end.},
	author = {J.C. Butcher},
	doi = {https://doi.org/10.1016/S0377-0427(00)00455-6},
	issn = {0377-0427},
	journal = {Journal of Computational and Applied Mathematics},
	keywords = {Initial value problems, Adams--Bashforth method, Adams--Moulton method, Runge--Kutta method, Consistency, Stability and convergence, Order of methods, Stiff problems, Differential equation software},
	note = {Numerical Analysis 2000. Vol. VI: Ordinary Differential Equations and Integral Equations},
	number = {1},
	pages = {1-29},
	title = {Numerical methods for ordinary differential equations in the 20th century},
	url = {https://www.sciencedirect.com/science/article/pii/S0377042700004556},
	volume = {125},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0377042700004556},
	bdsk-url-2 = {https://doi.org/10.1016/S0377-0427(00)00455-6}}

@book{Hairer1,
	added-at = {2010-10-02T18:22:22.000+0200},
	address = {Berlin},
	author = {Hairer, E. and N{\o}rsett, S.P. and Wanner, G.},
	biburl = {https://www.bibsonomy.org/bibtex/26bfd1a0356243229b8d30cb296e19f48/brouder},
	edition = {Second},
	interhash = {e4299d1b8f9819d82d6653a13cb75c5b},
	intrahash = {6bfd1a0356243229b8d30cb296e19f48},
	keywords = {imported},
	publisher = {Springer},
	timestamp = {2010-10-02T18:22:26.000+0200},
	title = {Solving Ordinary Differential Equations {I} Nonstiff problems},
	year = 2000}

@book{Hairer2,
	author = {Hairer, Ernst and Wanner, Gerhard},
	doi = {10.1007/978-3-662-09947-6},
	journal = {Springer Verlag Series in Comput. Math.},
	month = {01},
	title = {Solving Ordinary Differential Equations II. Stiff and Differential-Algebraic Problems},
	volume = {14},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-662-09947-6}}

@book{NocedalOptimization,
	added-at = {2009-08-21T12:21:08.000+0200},
	address = {New York, NY},
	author = {Nocedal, {Jorge} and Wright, {Stephen J.}},
	biburl = {https://www.bibsonomy.org/bibtex/28a42f1264dbca5b2e10460f70802807e/fbw_hannover},
	edition = {2. ed.},
	interhash = {22a7fec4243462045dfaabf3a92ff93f},
	intrahash = {8a42f1264dbca5b2e10460f70802807e},
	isbn = {978-0-387-30303-1},
	keywords = {Mathematical_optimization Mathematische_Optimierung Methoden_und_Techniken_der_Betriebswirtschaft Methoden_und_Techniken_der_Ingenieurwissenschaften Numerische_Mathematik Numerisches_Verfahren Optimierung Theorie},
	pagetotal = {XXII, 664},
	ppn_gvk = {502988711},
	publisher = {Springer},
	series = {Springer series in operations research and financial engineering},
	timestamp = {2009-08-21T12:21:09.000+0200},
	title = {Numerical optimization},
	url = {http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+502988711&sourceid=fbw_bibsonomy},
	year = 2006,
	bdsk-url-1 = {http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+502988711&sourceid=fbw_bibsonomy}}

@misc{ReddiAdamConvergence,
	author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1904.09237},
	keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
	publisher = {arXiv},
	title = {On the Convergence of Adam and Beyond},
	url = {https://arxiv.org/abs/1904.09237},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1904.09237},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1904.09237}}

@article{BUTCHER2000,
	abstract = {Numerical methods for the solution of initial value problems in ordinary differential equations made enormous progress during the 20th century for several reasons. The first reasons lie in the impetus that was given to the subject in the concluding years of the previous century by the seminal papers of Bashforth and Adams for linear multistep methods and Runge for Runge--Kutta methods. Other reasons, which of course apply to numerical analysis in general, are in the invention of electronic computers half way through the century and the needs in mathematical modelling of efficient numerical algorithms as an alternative to classical methods of applied mathematics. This survey paper follows many of the main strands in the developments of these methods, both for general problems, stiff systems, and for many of the special problem types that have been gaining in significance as the century draws to an end.},
	author = {J.C. Butcher},
	doi = {https://doi.org/10.1016/S0377-0427(00)00455-6},
	issn = {0377-0427},
	journal = {Journal of Computational and Applied Mathematics},
	keywords = {Initial value problems, Adams--Bashforth method, Adams--Moulton method, Runge--Kutta method, Consistency, Stability and convergence, Order of methods, Stiff problems, Differential equation software},
	note = {Numerical Analysis 2000. Vol. VI: Ordinary Differential Equations and Integral Equations},
	number = {1},
	pages = {1-29},
	title = {Numerical methods for ordinary differential equations in the 20th century},
	url = {https://www.sciencedirect.com/science/article/pii/S0377042700004556},
	volume = {125},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0377042700004556},
	bdsk-url-2 = {https://doi.org/10.1016/S0377-0427(00)00455-6}}

@article{stiffequation,
	author = {C. F. Curtiss and J. O. Hirschfelder},
	doi = {10.1073/pnas.38.3.235},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.38.3.235},
	journal = {Proceedings of the National Academy of Sciences},
	number = {3},
	pages = {235-243},
	title = {Integration of Stiff Equations*},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.38.3.235},
	volume = {38},
	year = {1952},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.38.3.235},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.38.3.235}}

@article{IMEXpaper,
	abstract = {Implicit-explicit (IMEX) linear multistep time-discretization schemes for partial differential equations have proved useful in many applications. However, they tend to have undesirable time-step restrictions when applied to convection-diffusion problems, unless diffusion strongly dominates and an appropriate BDF-based scheme is selected (Ascher et al., 1995). In this paper, we develop Runge-Kutta-based IMEX schemes that have better stability regions than the best known IMEX multistep schemes over a wide parameter range.},
	author = {Uri M. Ascher and Steven J. Ruuth and Raymond J. Spiteri},
	doi = {https://doi.org/10.1016/S0168-9274(97)00056-1},
	issn = {0168-9274},
	journal = {Applied Numerical Mathematics},
	note = {Special Issue on Time Integration},
	number = {2},
	pages = {151-167},
	title = {Implicit-explicit Runge-Kutta methods for time-dependent partial differential equations},
	url = {https://www.sciencedirect.com/science/article/pii/S0168927497000561},
	volume = {25},
	year = {1997},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0168927497000561},
	bdsk-url-2 = {https://doi.org/10.1016/S0168-9274(97)00056-1}}

@article{ARKmethods,
	abstract = {Additive Runge--Kutta (ARK) methods are investigated for application to the spatially discretized one-dimensional convection--diffusion--reaction (CDR) equations. Accuracy, stability, conservation, and dense-output are first considered for the general case when N different Runge--Kutta methods are grouped into a single composite method. Then, implicit--explicit, (N=2), additive Runge--Kutta (ARK2) methods from third- to fifth-order are presented that allow for integration of stiff terms by an L-stable, stiffly-accurate explicit, singly diagonally implicit Runge--Kutta (ESDIRK) method while the nonstiff terms are integrated with a traditional explicit Runge--Kutta method (ERK). Coupling error terms of the partitioned method are of equal order to those of the elemental methods. Derived ARK2 methods have vanishing stability functions for very large values of the stiff scaled eigenvalue, z[I]→−∞, and retain high stability efficiency in the absence of stiffness, z[I]→0. Extrapolation-type stage-value predictors are provided based on dense-output formulae. Optimized methods minimize both leading order ARK2 error terms and Butcher coefficient magnitudes as well as maximize conservation properties. Numerical tests of the new schemes on a CDR problem show negligible stiffness leakage and near classical order convergence rates. However, tests on three simple singular-perturbation problems reveal generally predictable order reduction. Error control is best managed with a PID-controller. While results for the fifth-order method are disappointing, both the new third- and fourth-order methods are at least as efficient as existing ARK2 methods.},
	author = {Christopher A. Kennedy and Mark H. Carpenter},
	doi = {https://doi.org/10.1016/S0168-9274(02)00138-1},
	issn = {0168-9274},
	journal = {Applied Numerical Mathematics},
	number = {1},
	pages = {139-181},
	title = {Additive Runge--Kutta schemes for convection--diffusion--reaction equations},
	url = {https://www.sciencedirect.com/science/article/pii/S0168927402001381},
	volume = {44},
	year = {2003},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0168927402001381},
	bdsk-url-2 = {https://doi.org/10.1016/S0168-9274(02)00138-1}}

@article{crank_nicolson_1947,
	author = {Crank, J. and Nicolson, P.},
	doi = {10.1017/S0305004100023197},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	number = {1},
	pages = {50--67},
	publisher = {Cambridge University Press},
	title = {A practical method for numerical evaluation of solutions of partial differential equations of the heat-conduction type},
	volume = {43},
	year = {1947},
	bdsk-url-1 = {https://doi.org/10.1017/S0305004100023197}}

@inproceedings{SgdGeneralizeBetter,
	author = {Hardt, Moritz and Recht, Ben and Singer, Yoram},
	booktitle = {International conference on machine learning},
	organization = {PMLR},
	pages = {1225--1234},
	title = {Train faster, generalize better: Stability of stochastic gradient descent},
	year = {2016}}

@article{LargeBatchGeneralizeWorse,
	author = {Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
	journal = {arXiv preprint arXiv:1808.07217},
	title = {Don't use large mini-batches, use local sgd},
	year = {2018}}

@misc{AdamCloserSGD,
	author = {Gupta, Aman and Ramanath, Rohan and Shi, Jun and Keerthi, S Sathiya},
	title = {Adam vs. SGD: Closing the generalization gap on image classification},
	year = {2021}}

@misc{LargeBatchTraining,
	author = {You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
	copyright = {arXiv.org perpetual, non-exclusive license},
	doi = {10.48550/ARXIV.1904.00962},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	publisher = {arXiv},
	title = {Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
	url = {https://arxiv.org/abs/1904.00962},
	year = {2019},
	bdsk-url-1 = {https://arxiv.org/abs/1904.00962},
	bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1904.00962}}

@inproceedings{Adaptive_Nonconvex_Optimization,
	author = {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Adaptive Methods for Nonconvex Optimization},
	url = {https://proceedings.neurips.cc/paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2018/file/90365351ccc7437a1309dc64e4db32a3-Paper.pdf}}


@misc{NesterovODE,
  doi = {10.48550/ARXIV.1503.01243},
  
  url = {https://arxiv.org/abs/1503.01243},
  
  author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel J.},
  
  keywords = {Machine Learning (stat.ML), Classical Analysis and ODEs (math.CA), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{SSP1,
author = {Shu, Chi-Wang},
title = {Total-Variation-Diminishing Time Discretizations},
journal = {SIAM Journal on Scientific and Statistical Computing},
volume = {9},
number = {6},
pages = {1073-1084},
year = {1988},
doi = {10.1137/0909073},

URL = { 
        https://doi.org/10.1137/0909073
    
},
eprint = { 
        https://doi.org/10.1137/0909073
    
}
,
    abstract = { In the computation of conservation laws \$u\_t + f(u)\_x = 0\$, total-variation-diminishing (TVD) schemes have been very successful. Many TVD schemes are of method-of-lines form (i.e., discretized in spatial variables only); hence time discretizations that keep TVD and have other properties (e.g., large CFL numbers for steady state calculations, or high-order accuracy for time-dependent problems) are desirable. In this paper we present a class of m-step Runge–Kutta-type TVD time discretizations with large CFL number m, suitable for steady state calculations, and a class of multilevel type TVD high-order time discretizations suitable for time-dependent problems. Some preliminary numerical results are also given. }
}


@ARTICLE{SSP2,
    author = {{Shu}, Chi-Wang and {Osher}, Stanley},
    title = "{Efficient Implementation of Essentially Non-oscillatory Shock-Capturing Schemes}",
    journal = {Journal of Computational Physics},
    year = 1988,
    month = aug,
    volume = {77},
    number = {2},
    pages = {439-471},
    doi = {10.1016/0021-9991(88)90177-5},
    adsurl = {https://ui.adsabs.harvard.edu/abs/1988JCoPh..77..439S},
    adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{local_minima_backpropogation,
author = {Gori, Marco and Tesi, Alberto},
title = {On the Problem of Local Minima in Backpropagation},
year = {1992},
issue_date = {January 1992},
publisher = {IEEE Computer Society},
address = {USA},
volume = {14},
number = {1},
issn = {0162-8828},
url = {https://doi.org/10.1109/34.107014},
doi = {10.1109/34.107014},
abstract = {The authors propose a theoretical framework for backpropagation (BP) in order to identify some of its limitations as a general learning procedure and the reasons for its success in several experiments on pattern recognition. The first important conclusion is that examples can be found in which BP gets stuck in local minima. A simple example in which BP can get stuck during gradient descent without having learned the entire training set is presented. This example guarantees the existence of a solution with null cost. Some conditions on the network architecture and the learning environment that ensure the convergence of the BP algorithm are proposed. It is proven in particular that the convergence holds if the classes are linearly separable. In this case, the experience gained in several experiments shows that multilayered neural networks (MLNs) exceed perceptrons in generalization to new examples.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {jan},
pages = {76–86},
numpages = {11},
keywords = {pattern recognition, convergence, perceptrons, learning systems, network architecture, local minima, neural nets, multilayered neural networks, backpropagation}
}


@misc{SGD_escapes_local_minima,
  doi = {10.48550/ARXIV.1802.06175},
  
  url = {https://arxiv.org/abs/1802.06175},
  
  author = {Kleinberg, Robert and Li, Yuanzhi and Yuan, Yang},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {An Alternative View: When Does SGD Escape Local Minima?},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{RELU_improve_Boltzman_machine,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}


@INPROCEEDINGS{best_multi-stage_architecture_for_object_recognition_LeCun,
  author={Jarrett, Kevin and Kavukcuoglu, Koray and Ranzato, Marc'Aurelio and LeCun, Yann},
  booktitle={2009 IEEE 12th International Conference on Computer Vision}, 
  title={What is the best multi-stage architecture for object recognition?}, 
  year={2009},
  volume={},
  number={},
  pages={2146-2153},
  doi={10.1109/ICCV.2009.5459469}}


@article{krizhevsky2017imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={AcM New York, NY, USA}
}


@book{time_integration_Ascher,
author = {Ascher, Uri M. and Petzold, Linda R.},
title = {Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations},
year = {1998},
isbn = {0898714125},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
edition = {1st},
abstract = {From the Publisher:Designed for those people who want to gain a practical knowledge of modern techniques, this book contains all the material necessary for a course on the numerical solution of differential equations. Written by two of the field's leading authorities, it provides a unified presentation of initial value and boundary value problems in ODEs as well as differential-algebraic equations. The approach is aimed at a thorough understanding of the issues and methods for practical computation while avoiding an extensive theorem-proof type of exposition. It also addresses reasons why existing software succeeds or fails. This book is a practical and mathematically well informed introduction that emphasizes basic methods and theory, issues in the use and development of mathematical software, and examples from scientific engineering applications. Topics requiring an extensive amount of mathematical development, such as symplectic methods for Hamiltonian systems, are introduced, motivated, and included in the exercises, but a complete and rigorous mathematical presentation is referenced rather than included. Audience This book is appropriate for senior undergraduate or beginning graduate students with a computational focus and practicing engineers and scientists who want to learn about computational differential equations. A beginning course in numerical analysis is needed, and a beginning course in ordinary differential equations would be helpful. About the Authors Uri M. Ascher is a Professor in the Department of Computer Science at the University of British Columbia, Vancouver. He is also Director of the Institute of Applied Mathematics there. Linda R. Petzold is a Professor in the Departments of Mechanical and Environmental Engineering and Computer Science at the University of California at Santa Barbara. She is also Director of the Computational Science and Engineering Program there.}
}


@InProceedings{deep_residual_network,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}


@article{lstm,
Abstract = {Addresses the problem of storing information over extended time intervals by recurrent backpropagation. Introduction of a novel, efficient, gradient-based method called long short-term memory; Results of experiments with artificial data.},
Author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
ISSN = {08997667},
Journal = {Neural Computation},
Keywords = {Information retrieval, Back propagation, Short-term memory},
Number = {8},
Pages = {1735},
Title = {Long short-term memory.},
Volume = {9},
URL = {http://login.ezproxy.lib.vt.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=iih&AN=9710215021&scope=site},
Year = {1997},
}


@inproceedings{attention_is_all_you_need,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929--1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}


@misc{auto_encoding_var_bias,
  doi = {10.48550/ARXIV.1312.6114},
  
  url = {https://arxiv.org/abs/1312.6114},
  
  author = {Kingma, Diederik P and Welling, Max},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Auto-Encoding Variational Bayes},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{gpt3,
  doi = {10.48550/ARXIV.2005.14165},
  
  url = {https://arxiv.org/abs/2005.14165},
  
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Models are Few-Shot Learners},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{VGG,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@software{otp,
title        = {{ComputationalScienceLaboratory/ODE-Test-Problems: 
                   v0.0.1}},
  author       = {Steven Roberts and
                  Andrey A Popov and
                  Arash Sarshar and
                  Reid Gomillion and
                  Adrian Sandu},
  month        = jun,
  year         = 2022,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.6676706},
  url          = {https://doi.org/10.5281/zenodo.6676706}
}
